# Metrics in Kubernetes

## Time-Series Data

Most metrics that we're interested in for cloud native observability purposes are expressed as time series.

> Metrics in cloud native applications are expressed numerically as time series.

## Counters and Gauges

There are two main types of metric values: _counters_ and _gauges_.

**Counters** can only go up or reset to zero. They are switable for measuring things like number of requests served and number of errors received

**Gauges** can vary up and down. They are useful for continuously varying quantities like memory usage or expressing rations of other quantities.

The answers to some questions are just yes or no. In this case, the appropiate metric will be a gauge with a limited range of values: 0 and 1.

```txt
For example, an HTTP chjeck of an endpoint might be named something like http.can_connect, and its value might be 1 when the endpoint is responding and 0 otherwise.
```

## What can metric tell us?

- Metrics can tell you when things are brotken. For example error rate suddenly goes up. You can generate alerts automatically for certain metrics based on a threshold.
- Metrics can also tell you **how well** things are working. For example how many simultaneous users your appolication is currently supporting.

## Choosing Metrics

> You can't monitor everything.

You need to focus on the subset of metrics that we care about. Only you can answer what metrics focus on when observing your own applications, buy we have a frew suggestions that may be helpful.

## Services: RED Pattern

For request-driven systems is useful to know about

- Number of requests 
- Number of requests failed (errors)
- Duration of each request 

The Requests-Errors-Duration (RED) pattern is a classic observability tool that goes back to the earliest dayts of online services. 

```txt
Why measure the same metrics for every service?

The benefits of treating each service the same, from monitoring perspective, is scalability in the operations team. By making every service look, feel and taste the same, this reduces the cognitive load on those responding to an incident. Also trating all your services the same, many repetitive tasks become automatable.
```

RED Pattern gives you useful information about how your services are performing and how users are experiencing them. This is a top-down way of looking at observability data.

### How measure these numbers

Since the total number of requests only ever goes up, it is more useful to look at request rate. This gives us a meaningful idea of how much traffic the system is hyandling over a given time interval.

Because error rate is related to request rate, it is a good idea to measure erros as a percentage of requests.

```txt
For example a typical service dashboard might show

- Requests received per second/minute
- Percentage of requests that returned an error
- Duration of requests (latency)
```

## Resources: The USE Pattern

On the other hand, USE pattern is a bottom-up approach that is intented to help analyze performance issues and find _bottlenecks_.

With Utilization, Saturation and Errors (USE) pattern, we are interested in _resources_. Lower-level infrastructure server components such as CPU, disks, or netowrk interfaces and links. Any these could be a bottleneck in system performance.

- _utilization_: The average time that the resource was buty serving requests, or the amount of resource capacity that is currently in use.
- _saturation_: The extent to which the resource is overloaded, or the length of the queue of requests waiting for this resource to become available.
- _errors_: number of times an operation on that resource failed.

Measuring data in key resources in your system is a good way to spot bottlenecks and potential upcoming problems.

> The strength of USE is its speed and visibility.

## Business Metrics

There are useful business metrics that can be generated by aplications and services. For example, suscription business, needs to know data about its suscribers.

- Funnel analytics (how many people hit the landing page, how many click through the sign-up page, how many complete the transaction, and so on)
- Rate of sign-ups and cancellations
- Revenue per customer
- Effectiveness of help and support pages
- Traffic to the system status annoucement page.

It's wise to discuss metrics at an early stage with all the stakeholders involved, and agree on what data needs to be collected, how often, how it's aggregated, and so on.

## Kubernetes Metrics 

To monitor the health and performance of the cluster at the top level, you should me looking at least at the following

- number of nodes
- node health status
- number of pods per node, and overall
- resource usage/allocation per node, and overall

### Deploying Metrics

- Number of deployments
- Number of configured replicas per deployment
- Number of unavailable replicas per deployment

### Container Metrics 

- Number of container/Pods per node, and overall
- Resource usage for each container agains its requests/limits 
- Liveness/readiness of containers
- Numbner of container/Pod restarts
- Network in/out traffic and erros for each container

### Application Metrics 

These are p[rimarly useful for developers and operations teams to be able to see what the application is doingit, and how long it takes. These are key indicators of performance problems or avaiability resources.

- Requests received
- Errors returned
- Duration
- Number of actions generated
- Nuimber of failed actions

It can be difficult to know what metrics are going to be useful at an early stage of development. Metrics are relatively cvheap for most applications to output and for time-series databases to store.

> If it moves, graph it. Even it dows not move, graph it anyway, because it might someday

### Runtime Metrics

- Number of processes/threads/goroutines
- heap and stack usage
- nonheap memory usage
- network i/o buffer pools
- garbage collector runs and pause durations
- file descriptors/network sockets in use

This kind of information can be very valuable for diagnosing poor performance or even crashes.

## Analyzing Metrics

In order tro get useful information out of the raw data we've captured we need to aggregate, process, and analyze it, which means doing _statistics_ on it.

### Applying Percentiles to Metrics Data

[https://igor.io/latency]

### We Usually want to know the worst

The P99 data is likely to give us a more eralistic picture of the latency experienced by users. For example, consider a high-traffic website with 1 million page views per day. If the P99 latency is 10 seconds, then 10.000 page vies take longer than 10 seconds. That's a lot of unhappy users

### Betyond Percentiles

One problem with percentiles latencies as implemented by many metrics services, is that requests tend to be spampled localy and statistics thenm aggregated centrally. Since percentile is already an average, and trying to average average is a well known statistical trap [https://oreil.ly/ZHCMU].

A better approach is to monitor what is wrong, not what is right. Instead of percentiles as primary metric to monitor performance. An option is monitor the percentage of requests that are over the threshold. To alert when SLAs are violated, we can trigger alarms when that percentage is grater than 1% over some predefined time window.

## Grahing Metrics with Dashboards

### 1. Use a Standard Layout for All Services

It makes sense to always lay out your dashboards in the same way for each service.

- One row per service
- Request and error rate on the left, with errors as percentage of requests
- Latencyt on the right

It is important thing that you use the same layout for every dashboard, and everyone is familiar with it.

- Requests, errors, duration dashboards works well for services (RED Pattern)
- For resources such as cluster nodes, disks, and networks the most useful things to nkow are utilization, saturation, errors.

### 2. Primary Dashboards

If you have a hundred of services, you have a hundred of dashboards. buy you probably won't look at them very often. It is still important to have that information available, but at this scale you need a more general overview.

To do this **make a primary dashboard** that shows requests, errors and duration across **all** your services. Stick to simple line graphs of total requests, total error percentage and total latency. These are easier to interpret and more accurate visualizations than complex charts.

For distributied teams, the information radiator (wallboard) could be the homepage of the monitoring website that everyone sees when they first log in. The purpose of an information radiator is:

- To show the current system status at a glance
- To send a clear message about which metrics the team considers important
- Make people familiar with what _normal_ looks like

On radiator screen you should include only vital information. _Vital_ in the sense of _really important_ buy also in sense of _vital signs_: information that tells you about the life of the system.

Similary to a hospital bed, your information should show the vital signs of your business or service. The goal is to focus on a few key things and make them easily visible from across a room.

### 3. Things that break dashboards

Create dashboards for individual services and resources. You may want to create dashboards for specific metrics that tell you important things about the system. The useful source information is things that break.

Every time you have an incident or outage, look for a metric or combination of metrics, which would have alerted you to this problem in advance.

We are not talking here about problems that happen over a period of minutes or even hours; those are usually caught by automated alerts. Rather, we are interested in the slow-moving icebergs that draw closer over days or weeks.

After an incident, always ask "What would have warned us about this problem in advance, if only we'd been aware of it?". If the answer is data you already had buyt did not pay attention to, take action to highlight that data.

## Alerting on Metrics

Alerts indicate some unexpected deviation from a stable, working state. Buyt distributed systems don't have those states. Large-scale distributed systems are never completely up, they are almost always in a state of partially degraded service. 

For a monitoring system to be useful, it has to have a very high signal to noise ratio. False alamrgs are not only annoying, but dangerous: they reduce trust in the system, and condition people that alerts can be safely ignored.

[https://oreil.ly/cXEOk] 

> **An alert should mean one veryh simple thing: action needs to be taken now**

If action needs to happen sometime, but nowt right nowm, the alert can be downgraded to a lower priority notification, like an email or chat message. If the action can be taken by an automated system, then automate it. Don't wake up a valuable human being.

### Urgent, Important and Actionable Alerts

Pages should be restricted to only urgent, important and actionable alerts.

- Alerts that are important but not urgent, can be dealt with during normal working hours.
- Alerts that are urgent, but not important, don't justify waking someone up.
- If there is no immediate action that can be taken to fix it, there's no point paging about it.
